robots.txt 
:	사이트의 루트에 위치하며 사이트에서 검색 엔진 크롤러가 액세스하지 않기를 바라는 부분을 표시합니다. 파일에서는 로봇 배제 표준 프로토콜의 명령을 사용하여 섹션별, 웹 크롤러 종류별(모바일 크롤러, 데스크톱 크롤러 등)로 사이트에 대한 액세스 권한을 표시합니다.


##robots.txt의 용도는?
비이미지 파일
비이미지 파일(즉, 웹페이지) robots.txt는 크롤링 트래픽을 제어할 목적으로만 사용되어야 합니다. 일반적으로 서버가 Google의 크롤러에 의해 과부하 상태가 되거나, 사이트에서 중요하지 않거나 유사한 페이지를 크롤링하면서 크롤링 예산을 낭비하고 싶지 않을 것이기 때문입니다. robots.txt를 Google 검색결과에서 웹페이지를 숨기기 위한 수단으로 사용하지 말아야 합니다. 다른 페이지가 내 페이지로 연결되어 내 페이지에서 robots.txt 파일을 사용하지 않는 방식으로 색인이 생성될 수 있기 때문입니다. 검색결과에서 페이지를 차단하려면 비밀번호 보호 또는 NOINDEX 태그 또는 명령어를 사용하세요.

##이미지 파일
robots.txt는 Google 검색결과에서 이미지 파일이 표시되지 않도록 합니다. (하지만 다른 페이지 또는 사용자가 내 이미지로 연결하지 못하도록 하는 것은 아닙니다.)

##리소스 파일
이러한 리소스 없이 로드된 페이지가 리소스 없이도 크게 영향을 받지 않는다고 생각되는 경우 robots.txt를 사용하여 중요하지 않은 이미지, 스크립트, 스타일 파일 등의 리소스 파일을 차단할 수 있습니다. 하지만 이러한 리소스가 없어 페이지에서 Google의 크롤러를 인지하기 어렵게 되는 경우에는 차단해서는 안 됩니다. 차단하면 Google에서 이러한 리소스에 의존하는 페이지를 제대로 분석할 수 없게 됩니다.

##robots.txt의 제한사항 이해
robots.txt를 작성하기 전에 이 URL 차단 메소드의 위험에 대해 알아야 합니다. 웹에서 내 URL이 검색되지 않도록 하는 다른 메커니즘을 고려할 수도 있습니다.

##Robots.txt 명령은 지침에 지나지 않습니다.
robots.txt 파일의 지침은 사이트에 대한 크롤러의 동작을 강요할 수 없으며, 사이트에 액세스하는 크롤러에 대한 지침 역할을 합니다. Googlebot 및 기타 잘 제작된 웹 크롤러는 robots.txt 파일의 지침을 준수하지만 그렇지 않은 크롤러도 있습니다. 그러므로 정보를 웹 크롤러로부터 안전하게 보호하려면 서버에서 비공개 파일을 비밀번호로 보호하는 등 다른 차단 메소드를 사용하는 것이 좋습니다.
크롤러마다 구문을 다르게 해석합니다.
잘 제작된 웹 크롤러는 robots.txt 파일의 지침을 따르지만 크롤러마다 지침을 다르게 해석할 수도 있습니다. 특정 지침을 이해하지 못하는 크롤러도 있으므로 다양한 웹 크롤러에 적용될 수 있는 적절한 구문을 알아야 합니다.
robots.txt 지침은 다른 사이트에서 내 URL을 참조하지 못하게 차단할 수 없습니다.
Google은 robots.txt를 통해 차단된 콘텐츠를 크롤링하거나 콘텐츠의 색인을 생성하지 않지만, 웹의 다른 곳에서 허용되지 않은 URL에 대한 정보를 찾아 색인을 생성할 수는 있습니다. 결과적으로 URL 주소뿐 아니라 사이트 링크의 앵커 텍스트와 같은 기타 공개 정보가 Google 검색결과에 여전히 표시될 수 있습니다. 서버에서 파일 비밀번호 보호 등의 다른 URL 차단 방법을 사용하거나 NOINDEX 메타태그 또는 응답 헤더를 사용하여 Google 검색결과에 내 URL이 전혀 표시되지 않도록 할 수 있습니다.